<!doctype html>
<html>
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <!-- bulma css template -->
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bulma@0.9.4/css/bulma.min.css">
  <!-- ionicons -->
  <script type="module" src="https://unpkg.com/ionicons@7.1.0/dist/ionicons/ionicons.esm.js"></script>
  <script nomodule src="https://unpkg.com/ionicons@7.1.0/dist/ionicons/ionicons.js"></script>
  <!-- model viewer -->
  <script type="module" src="https://ajax.googleapis.com/ajax/libs/model-viewer/3.1.1/model-viewer.min.js"></script>
  <title>
    WS3DPT
  </title>
  <link rel="icon" href="icon.ico">
</head>
<body>
  <section class="section">

  <div class="container has-text-centered">
    <!-- paper title -->
    <p class="title is-3"> Weakly-supervised 3D Pose Transfer with Keypoints </p>
    <!-- publication -->
    <p class="subtitle is-4"> ICCV 2023 </p>
    <!-- authors -->
    <p class="title is-5 mt-2"> 


       <a href="https://jinnan-chen.github.io/">Jinnan Chen</a><sup>1</sup>, 
       <a href="https://chaneyddtt.github.io/">Chen Li</a><sup>1</sup>,    
    <a href="https://www.comp.nus.edu.sg/~leegh/">Gim Hee Lee</a><sup>1</sup>, 
<!--       <a href="https://me.kiui.moe/" target="_blank">Jiaxiang Tang</a><sup>1</sup>, 
      <a href="https://frozenburning.github.io/" target="_blank">Zhaoxi Chen</a><sup>2</sup>, 
      <a href="https://charlescxk.github.io/" target="_blank">Xiaokang Chen</a><sup>1</sup>, 
      <a href="https://tengfei-wang.github.io/" target="_blank">Tengfei Wang</a><sup>3</sup>, 
      <a href="http://www.cis.pku.edu.cn/info/1177/1378.htm" target="_blank">Gang Zeng</a><sup>1</sup>,
      <a href="https://liuziwei7.github.io/" target="_blank">Ziwei Liu</a><sup>2</sup> -->
    </p>
    <!-- affiliations -->
    <p class="subtitle is-5"> 
      <sup>1</sup> National University of Singapore &nbsp;

<!--       <sup>3</sup> Shanghai AI Lab &nbsp; -->
    </p>

    <!-- other links -->
    <div class="is-flex is-justify-content-center">
      <span class="icon-text mx-1">
        <a class="button is-dark" href="https://arxiv.org/abs/2307.13459" role="button" target="_blank"> <span class="icon"> <ion-icon name="document-outline"></ion-icon> </span> <span> Arxiv </span>  </a> 
      </span>
      <span class="icon-text mx-1">
        <a class="button is-dark" href="https://github.com/jinnan-chen/3D-Pose-Transfer" role="button" target="_blank"> <span class="icon"> <ion-icon name="logo-github"></ion-icon> </span> <span> Code </span> </a> 
      </span>
     
    </div>
   
  </div>

  <!-- main container -->
  <div class="container is-max-desktop has-text-centered">

    <!-- abstract -->
    <p class="title is-3 mt-5 has-text-centered"> Abstract </p>
    <p class="content is-size-6 has-text-left">
        The main challenges of 3D pose transfer are: 1) Lack
        of paired training data with different characters perform-
        ing the same pose; 2) Disentangling pose and shape infor-
        mation from the target mesh; 3) Difficulty in applying to
        meshes with different topologies. We thus propose a novel
        weakly-supervised keypoint-based framework to overcome
        these difficulties. Specifically, we use a topology-agnostic
        keypoint detector with inverse kinematics to compute trans-
        formations between the source and target meshes. Our
        method only requires supervision on the keypoints, can be
        applied to meshes with different topologies and is shape-
        invariant for the target which allows extraction of pose-only
        information from the target meshes without transferring
        shape information. We further design a cycle reconstruction
        to perform self-supervised pose transfer without the need
        for ground truth deformed mesh with the same pose and
        shape as the target and source, respectively. We evaluate
        our approach on benchmark human and animal datasets,
        where we achieve superior performance compared to the
        state-of-the-art unsupervised approaches and even compa-
        rable performance with the fully supervised approaches. We
        test on the more challenging Mixamo dataset to verify our
        approachâ€™s ability in handling meshes with different topolo-
        gies and complex clothes. Cross-dataset evaluation further
        shows the strong generalization ability of our approach.
    </p>

  <!-- method -->
  <p class="title is-3 mt-5 has-text-centered"> Method </p>
  <p class="content is-size-6 has-text-left">
   <img src="img/framework.png" class="img-responsive">
    The overall framework of our proposed approach. The left part is our pipeline for pose transfer, which contains four learnable
    components: a keypoints detection network, a twist prediction network, a skinning weights prediction network, and a refinement network,
    and two functions: an Inverse and Forward Kinematics function and an LBS function. The right part is an illustration of the cycle
    reconstruction process. The yellow and blue meshes represent two different characters.
  </p> 
  <!-- Visualization -->
  <p class="title is-3 mt-5 has-text-centered"> Visualization </p>
  <p class="content is-size-6 has-text-left">
   <img src="img/3dvis.png" class="img-responsive">
    We show some results on stylized human meshes and animal meshes.
  </p> 
  <!-- Comparison -->
  <p class="title is-3 mt-5 has-text-centered">  Comparison </p>
  <p class="content is-size-6 has-text-left">
   <img src="img/com.png" class="img-responsive">
    We show comparison with <a href="https://github.com/zycliao/skeleton-free-pose-transfer">SKF</a> (ECCV 2022 paper Skeleton-free Pose Transfer for Stylized 3D Characters).
  </p> 
    
  <!-- citation -->
    <div class="card mt-4">
      <header class="card-header">
        <p class="card-header-title"> Citation </p>
      </header>
      <div class="card-content is-size-5 has-text-left">
<pre><code>@article{chen2023weaklysupervised,
  title={Weakly-supervised 3D Pose Transfer with Keypoints},
  author={Chen, Jinnan and Li, Chen and Lee, Gim Hee},
  journal={arXiv preprint arXiv:2307.13459},
  year={2023}
}
}</code></pre>
      </div>
    </div>
  </div>


  </section>
</body>
</html>
